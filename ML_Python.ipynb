{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "ML Python.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb_PBThH0Pri"
      },
      "source": [
        "# Einführung in Machine Learning mit Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnGR6T1Y0Prq"
      },
      "source": [
        "## Vektorisierung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FENxfRi0Prr"
      },
      "source": [
        "### Datensets mit Features - Beispiel Iris-Datenset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMMFjh3S0Prr"
      },
      "source": [
        "Einige Datensets sind bereits schon in `scikit-learn` eingebaut. Du brauchst dich nicht mehr darum zu kümmern, diese\n",
        "* herunterzuladen,\n",
        "* zu verifizieren und\n",
        "* zu vektorisieren"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2bqNtJU0Prs"
      },
      "source": [
        "Das [Iris-Datenset](https://en.wikipedia.org/wiki/Iris_flower_data_set) ist ein *Klassiker*:\n",
        "\n",
        "![image](https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/220px-Kosaciec_szczecinkowaty_Iris_setosa.jpg) ![image](https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/220px-Iris_versicolor_3.jpg) ![image](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/220px-Iris_virginica.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJasAly30Prt"
      },
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3y1l-y20Prt"
      },
      "source": [
        "Leider in einem etwas sperrigen Format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEyZhPgy0Prt"
      },
      "source": [
        "iris"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSHB673p0Prv"
      },
      "source": [
        "Besser als `DataFrame` mit `pandas`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emU_ft_c0Prv"
      },
      "source": [
        "import pandas as pd\n",
        "idf = pd.DataFrame(iris[\"data\"], columns=[\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\"])\n",
        "idf[\"target\"] = iris[\"target\"]\n",
        "idf[\"name\"] = [iris[\"target_names\"][target] for target in iris[\"target\"]]\n",
        "idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQRX8RUq0Prw"
      },
      "source": [
        "*Five Number Summaries*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4poK7D6w0Prw"
      },
      "source": [
        "idf.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDNo5Ny90Prw"
      },
      "source": [
        "Verteilungen als Histogramm plotten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfrDV0pB0Prw"
      },
      "source": [
        "idf[\"Petal Width\"].plot.hist(bins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFv3gJVl0Prx"
      },
      "source": [
        "Überlegung, welche Features sich besonders zur Differenzierung eignen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI2z9_xH0Prx"
      },
      "source": [
        "idf.plot.scatter(x=\"Sepal Length\", y=\"Sepal Width\", c=\"target\", cmap=\"Set1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oD4ov-L0Prx"
      },
      "source": [
        "Oder alle auf einmal? Hier erkennst du mögliche Korrelationen oder in welchen Dimensionen die Objekte sich am deutlichsten unterscheiden:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTRRSwdd0Prx"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.pairplot(idf.drop(columns=[\"target\"]), hue=\"name\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5q2F00O0Pry"
      },
      "source": [
        "### Unstrukturierte Datensets - Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRvS1KLp0Pry"
      },
      "source": [
        "Bei unstrukturierten Daten musst du erst Features finden. Bei Bildern ist das z.B. sehr schwierig, deswegen wird dort fast immer nur Deep Learning verwendet. Dort macht der erste Layer aus den Pixel eine Art Features.\n",
        "\n",
        "Wir beschäftigen und jetzt mit Textdaten. Dazu haben wir alle Toplevel-Posts des [Technology Subreddit](https://www.reddit.com/r/Technology) heruntergeladen und in einer CSV-Datei gespeichert:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HuPGQO20Pry"
      },
      "source": [
        "docs = pd.read_csv(\"https://github.com/datanizing/heise-webinar-ml-python/raw/master/reddit-technology-toplevel-title.csv.gz\", \r\n",
        "                   parse_dates=[\"created_utc\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpRii6_O0Pry"
      },
      "source": [
        "Wie du siehst, sind das *viele* Posts, nämlich fast zwei Millionen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuD25gL50Prz"
      },
      "source": [
        "docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T9ZIg2DSPeL"
      },
      "source": [
        "docs.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDl7E1QY0Prz"
      },
      "source": [
        "Sind die Daten hinreichend aktuell? Solche Informationen überprüfst du besser gleich ganz zu Beginn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utg0unJl0Pr0"
      },
      "source": [
        "docs.set_index(\"created_utc\").resample(\"Q\").count().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v8Ajx4l0Pr0"
      },
      "source": [
        "Das ist zwar nicht perfekt, aber es eignet sich doch hinreichend gut für eine Analyse.\n",
        "\n",
        "Um die Daten zu vektorisieren, solltest du zuerst die Stopwords eliminieren. Das ist normalerweise ein iterativer Prozess, hier findest du die etwas ergänzte Stopword-Liste von `spacy` (damit du das nicht noch installieren musst):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq5J3J2e0Pr0"
      },
      "source": [
        "stop_words= {\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', \n",
        "             'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', \n",
        "             'amongst', 'amount', 'amp', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', \n",
        "             'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', \n",
        "             'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', \n",
        "             'beyond', 'blog', 'body', 'both', 'bottom', 'but', 'buy', 'buycheap', 'by', 'ca', 'call', 'can', 'cannot', \n",
        "             'case', 'change', 'co', 'com', 'could', 'create', 'delete', 'did', 'do', 'does', 'doing', 'done', 'down', \n",
        "             'download', 'drive', 'due', 'during', 'each', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'email', \n",
        "             'empty', 'enough', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', \n",
        "             'fifteen', 'fifty', 'first', 'five', 'fix', 'for', 'former', 'formerly', 'forty', 'four', 'from', 'front', \n",
        "             'full', 'further', 'get', 'give', 'go', 'good', 'had', 'has', 'have', 'he', 'help', 'hence', 'her', 'here', \n",
        "             'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', \n",
        "             'http', 'https', 'hundred', 'i', 'if', 'in', 'indeed', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', \n",
        "             'last', 'late', 'latter', 'latterly', 'least', 'less', 'll', 'look', 'made', 'make', 'many', 'market', 'may', \n",
        "             'me', 'meanwhile', 'message', 'might', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', \n",
        "             'must', 'my', 'myself', \"n't\", 'name', 'namely', 'need', 'neither', 'never', 'nevertheless', 'new', 'news', \n",
        "             'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'number', 'n‘t', \n",
        "             'n’t', 'of', 'off', 'often', 'on', 'once', 'one', 'online', 'only', 'onto', 'or', 'other', 'others', \n",
        "             'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'page', 'part', 'pass', 'per', 'perhaps', \n",
        "             'please', 'post', 'put', 'question', 'quite', 'rather', 're', 'really', 'reddit', 'regarding', 'remove', \n",
        "             'review', 'same', 'say', 'search', 'see', 'seem', 'seemed', 'seeming', 'seems', 'self', 'send', 'serious', \n",
        "             'several', 'she', 'should', 'show', 'side', 'since', 'site', 'six', 'sixty', 'so', 'some', 'somehow', \n",
        "             'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'support', 'take', 'ten', \n",
        "             'test', 'text', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', \n",
        "             'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'third', 'this', 'those', \n",
        "             'though', 'three', 'through', 'throughout', 'thru', 'thus', 'time', 'to', 'together', 'too', 'top', \n",
        "             'toward', 'towards', 'twelve', 'twenty', 'two', 'under', 'unless', 'unlock', 'until', 'up', 'upon', \n",
        "             'us', 'use', 'used', 'using', 'various', 've', 'very', 'via', 'video', 'was', 'watch', 'way', 'we', 'well', \n",
        "             'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', \n",
        "             'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', \n",
        "             'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'work', 'would', 'yet', 'you', 'your', \n",
        "             'yours', 'yourself', 'yourselves', '‘d', '‘ll', '‘m', '‘re', '‘s', '‘ve', '’d', '’ll', '’m', '’re', \n",
        "             '’s', '’ve'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXTX4GFF0Pr2"
      },
      "source": [
        "Für die Vektorisierung von Dokumente werden normalerweise die Wörter gezählt und als Features verwendet. Damit besonders häufige Wörter nicht zu stark dominieren, werden die mit der sog. \"Inverierten Dokumentenfrequenz\" abgewertet.\n",
        "\n",
        "`scikit-learn` kann das mit dem `TfidfVectorizer` alles für dich erledigen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MMl7E-q0Pr2"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=10, stop_words=stop_words)\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(docs[\"title\"].map(str))\n",
        "tfidf_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtdscgY-0Pr3"
      },
      "source": [
        "Wie du siehst, ist die entstehende Matrix mit fast zwei Millionen Zeilen und knap 45.000 Spalten *sehr groß*. Da die meisten Wörter in den meisten Dokumenten nicht vorkommen, besteht sie zu einem ganz großen Teil aus leeren Einträgen und kann daher als sog. *sparse matrix* sehr effizient abgelegt werden:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB2SgOeu0Pr3"
      },
      "source": [
        "tfidf_vectors.data.nbytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FwnKomg0Pr4"
      },
      "source": [
        "Bei einer `float`-Darstellung mit vier Bytes pro `float` würde das in einer naiven Darstellung 1922041 x 44980 x 4 Bytes = 322 GB RAM benötigen. Zum Glück sind es hier nur 130 MB - ein Effizienzgewinn fast um den Faktor 3.000. Nur deswegen funktioniert das überhaupt!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqmj47ML0Pr4"
      },
      "source": [
        "## Überwachte Lernverfahren"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvT4NprM0Pr4"
      },
      "source": [
        "Überwachte Lernverfahren benötigen Trainingsdaten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mub9qF0q0Pr4"
      },
      "source": [
        "### Iris-Klassifikation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5rNLGYX0Pr4"
      },
      "source": [
        "Zum Training nutzt du die Iris-Daten, von denen du das Ergebnis schon kennst. Als Modell verwendest du eine sog. [Support Vector Machine](https://de.wikipedia.org/wiki/Support_Vector_Machine). Diese funktioniert sehr effizient:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWBBQMEV0Pr7"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "svm = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)\n",
        "svm.fit(iris['data'], iris['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVtcQlKe0Pr7"
      },
      "source": [
        "Jetzt kannst du aus den Daten vorhersagen lassen, zu welcher Iris-Sorte eine Pflanze gehört, wenn du nur die Messwerte kennst."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y8mBude0Pr8"
      },
      "source": [
        "svm.predict(iris['data'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pvJQF300Pr8"
      },
      "source": [
        "Wie vergleicht sich das mit den echten Werten?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJYlHG3g0Pr8"
      },
      "source": [
        "svm.predict(iris['data']) == iris['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j1Fn69P0Pr8"
      },
      "source": [
        "Eigentlich sieht das ganz gut aus, aber es gibt mehrere Probleme:\n",
        "* Kann man die Performance des Klassifikators *messen*?\n",
        "* Hat der Klassifikator evtl. nur (fast) alles auswendig gelernt oder kann er tatsächlich *abstrahieren*?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4c62kIW0Pr8"
      },
      "source": [
        "### Trainings-Test-Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwSzY4CP0Pr9"
      },
      "source": [
        "Um das zu beheben, kannst du die vorklassifizierte Datenmenge in zwei Teile zerlegen: eine *Trainingsmenge*, die nur dem Training des Klassifikators dient und eine davon unabhängige *Testmenge*, mit der der Klassifikator beweisen kann, wie gut er abstrahieren kann.\n",
        "\n",
        "`scikit-learn` stellt dafür eine Funktion `train_test_split` zur Verfügung. Der `random_state` dient der Reproduzierbarkeit, mit `test_size` kann man das Verhältnis der beiden Mengengrößen wählen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4Lh17V50Pr9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], test_size = 0.25, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErcuMCS20Pr9"
      },
      "source": [
        "Wieder trainierst du die SVM als Klassifikator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGmeDGsQ0Pr9"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "svm = SVC(max_iter=1000, tol=1e-3, random_state=42)\n",
        "svm.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8fOqA4T0Pr-"
      },
      "source": [
        "Dieses Mal sagst du die Spezies für die Testdaten vorher:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52BxFyc_0Pr-"
      },
      "source": [
        "svm.predict(X_test) == y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggv2zGHe0Pr-"
      },
      "source": [
        "Zwei Werte sind falsch vorhergesagt. Welche das sind, findest du in der [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obtj87Hx0Pr-"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, svm.predict(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH52dhD40Pr_"
      },
      "source": [
        "Auf der Diagonalen stehen dort die richtig vorgergesagten Werte, die `2` außerhalb der Diagonale zeigt dir falsch vorhergesagte Ergebnisse. In diesem Fall wäre das richtige Ergebnis die Klasse `2` gewesen (2. Spalte), stattdessen wurde `3` vorhergesagt (3. Zeile).\n",
        "\n",
        "Oftmals möchtest du die Genauigkeit der Klassifikation messen. Dazu dienen gleich zwei Größen, nämlich die Precision (Spezifizität) und der Recall (Sensitivität):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWUhzgcL0Pr_"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, svm.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VHRmpz5bEr-"
      },
      "source": [
        "print(classification_report(y_train, svm.predict(X_train)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eRKaWiK0Pr_"
      },
      "source": [
        "Im Gegensatz zur häufig verwendeten *Accuracy* können Precision und Recall auch mit *schiefen* Verteilungen umgehen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSvBRglG0Pr_"
      },
      "source": [
        "### Reddit-Klassifikation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VNQmV9I0Pr_"
      },
      "source": [
        "Betrachten wir unser echtes Datenset, gibt es auch hier Kategorien, nämlich die Flairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LswcNL2V0Pr_"
      },
      "source": [
        "docs.groupby(\"flair\").agg({\"title\": \"count\"}).sort_values(\"title\", ascending=False).head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cyOsakp0PsA"
      },
      "source": [
        "Passen zum Thema des Webinars suchen wir uns den Flair *Artificial Intelligence* aus und setzen dessen *Target* auf 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RGpzry60PsA"
      },
      "source": [
        "docs[\"target\"] = 0\n",
        "docs.loc[docs[\"flair\"] == \"Artificial Intelligence\", \"target\"] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_tJYEdW0PsA"
      },
      "source": [
        "Nun sind allerdings die Dokumente ohne diesen Flair deutlich überrepräsentiert:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U20wbql90PsA"
      },
      "source": [
        "ai = docs[docs[\"flair\"] == \"Artificial Intelligence\"]\n",
        "len(ai)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL8vFefZ0PsA"
      },
      "source": [
        "non_ai = docs[(docs[\"flair\"] != \"Artificial Intelligence\") & \n",
        "              (docs[\"created_utc\"].dt.year >= 2015) &\n",
        "              ~docs[\"flair\"].isna()]\n",
        "len(non_ai)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmCghZ0e0PsB"
      },
      "source": [
        "Damit kann ein Modell nicht gut trainiert werden, weil viel zu viele negative Beispiele den Klassifikator in eine falsche Richtung drängen. Deswegen *stratifizieren* wir das Datenset und nehmen so viele negative wie positive Samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwV5FOAW0PsB"
      },
      "source": [
        "data = pd.concat([ai, non_ai.sample(n = len(ai), random_state=42)])\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjJc8HNv0PsB"
      },
      "source": [
        "Du kannst weiter mit dem bereits trainierten Vectorizer arbeiten und rufst daher nur dessen `transform`-Methode auf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb4t84yW0PsB"
      },
      "source": [
        "ai_tfidf_vectors = tfidf_vectorizer.transform(data[\"title\"].map(str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFwR34_h0PsB"
      },
      "source": [
        "Um die Ergebnisqualität messen zu können, teilst du das Datenset auf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h587Aje0PsB"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(ai_tfidf_vectors, data['target'], test_size = 0.25, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exMMV5-e0PsC"
      },
      "source": [
        "Der Klassifikator ist schnell trainiert:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "porrHSvj0PsC"
      },
      "source": [
        "ai_svm = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)\n",
        "ai_svm.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgczeU-o0PsC"
      },
      "source": [
        "Das Ergebnis ist schon ziemlich gut:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-zVqpoQ0PsC"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, ai_svm.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp_kUlpB0PsD"
      },
      "source": [
        "Mit 88% Genauigkeit (hier passt die Accuracy wegen dem gleichverteilten Datenset) kannst du jetzt vorhersagen, ob ein Post von AI handelt oder nicht."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypFF2rzd0PsD"
      },
      "source": [
        "### Reddit-Regression (Vorhersage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km3XBHRU0PsD"
      },
      "source": [
        "Jetzt willst du versuchen, den Trend in den AI-Meldungen zu ermitteln. Dazu aggregierst du zunächst die AI-Post und parallel dazu alle Posts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJZWVuFj0PsD"
      },
      "source": [
        "time_agg = docs.set_index(\"created_utc\").resample(\"M\").\\\n",
        "       agg({\"target\": \"sum\", \"title\": \"count\" }).rename(columns={\"title\": \"total\"})\n",
        "time_agg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-S7iDCm0PsE"
      },
      "source": [
        "Jetzt kannst du den relativen Anteil der AI-Posts ausrechnen. Nur diese lassen sich vorhersagen, weil sonst das Post-Volumen von Reddit noch mit vorhergesagt werden müsste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3ku2zym0PsE"
      },
      "source": [
        "time_agg[\"rel\"] = time_agg[\"target\"] / time_agg[\"total\"]\n",
        "time_agg[[\"rel\"]].plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXd47FX50PsE"
      },
      "source": [
        "Für eine Trendvorhersage erzeugst du einen neuen `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQD0d7N40PsE"
      },
      "source": [
        "pa = pd.DataFrame()\n",
        "pa[\"ds\"] = time_agg.index\n",
        "pa[\"y\"] = time_agg[\"rel\"].values\n",
        "pa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na5hCRqd0PsF"
      },
      "source": [
        "Flairs wurden erst ab 2015 vergeben, daher nutzt du nur diesen Zeitraum:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgiRJO900PsF"
      },
      "source": [
        "pa = pa[pa[\"ds\"] >= '2015-01-01']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hbKrtiB0PsF"
      },
      "source": [
        "Die einfachste Vorhersage ist eine lineare Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikgo_V7d0PsF"
      },
      "source": [
        "from scipy.stats import linregress\n",
        "linregress(x=pa[\"ds\"].index.values, y=pa[\"y\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA0CX0zZ0PsF"
      },
      "source": [
        "Das [Pearson R und P](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) sagen dir, dass hier eine relativ klare Korrelation vorliegt.\n",
        "\n",
        "Noch viel besser geht das allerdings mit [Prophet](https://facebook.github.io/prophet/):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ieX0PDQ0PsG"
      },
      "source": [
        "!pip install fbprophet\r\n",
        "from fbprophet import Prophet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwMYyXxT0PsG"
      },
      "source": [
        "Zuerst instanziierst du ein `Prophet` Objekt und übergibst ihm die Daten:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4t_Ya8z0PsG"
      },
      "source": [
        "m = Prophet()\n",
        "m.fit(pa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRk0cpjC0PsG"
      },
      "source": [
        "Dann kümmerst du dich um den Future-`DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ayk3sgLt0PsH"
      },
      "source": [
        "future = m.make_future_dataframe(periods=20, freq='M')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdHHPngx0PsH"
      },
      "source": [
        "Und führst die Vorhersagen durch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zTNsBAR0PsH"
      },
      "source": [
        "forecast = m.predict(future)\n",
        "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbv6ugQD0PsH"
      },
      "source": [
        "Besonders schön sind die dadurch entstehenden Plots:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fM95Yso0PsI"
      },
      "source": [
        "fig1 = m.plot(forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KNEj7oX0PsI"
      },
      "source": [
        "Auch die Analyse von Trend und Saisonalität ist aufschlussreich:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h-UdNRX0PsI"
      },
      "source": [
        "fig2 = m.plot_components(forecast)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUTnPtS40PsI"
      },
      "source": [
        "Wir sind zwar im richtigen Jahr unterwegs, aber leider gerade in einem saisonalen Downturn. Das ist allerdings mit Vorsicht zu genießen - bei AI gibt es keine Saisonalität (trotz [AI winter](https://en.wikipedia.org/wiki/AI_winter))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLNMQgb20PsI"
      },
      "source": [
        "## Unüberwachte Lernverfahren"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duwHunbQ0PsJ"
      },
      "source": [
        "Unüberwachte Lernverfahren ermitteln die *intrinsische Struktur* von Daten. Sie benötigen keine Trainingsdaten und sind unvoreingenommen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMtK448D0PsJ"
      },
      "source": [
        "### Iris-Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6K2L2bL0PsJ"
      },
      "source": [
        "Du kannst die Iris-Daten auch clustern. Die meisten Cluster-Algorithmen brauchen als Vorgabe die Anzahl der Cluster, so auch das hier verwendete `KMeans`. Versuche es erst mit einer \"falschen\" Anzahl an Clustern:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mfqo5DD10PsJ"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "km5 = KMeans(n_clusters=5)\n",
        "km5.fit(iris['data'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esyOqkQ70PsJ"
      },
      "source": [
        "In dem `labels_`-Feld stehen dir jetzt die entdecken Cluster zur Verfügung:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_2FLlOc0PsJ"
      },
      "source": [
        "km5.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgeQUJRj0PsK"
      },
      "source": [
        "Schau dir zum Vergleich die echten Varianten an:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCPN3Pd60PsK"
      },
      "source": [
        "iris['target']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWSfYFCB0PsK"
      },
      "source": [
        "Die Zahlen sind beliebig. Am Anfang hat es noch ganz gut geklappt - aber dann gibt es schon ein ziemliches Durcheinander!\n",
        "\n",
        "Oben hattet du gesehen, dass sich die Iris-Varianten am besten durch die *Petal Length* vs. *Sepal Length* unterscheiden lassen. Mit `seaborn` kannst du einen Scatter-Plot zeichnen lassen und sowohl die Marker als auch die Farben beeinflussen. Wenn du die Marker auf die echte Spezies setzt und die Farben auf die Cluster, siehst du, was nicht richtig gelöst wurde:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPQFeaZt0PsK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(16,9))\n",
        "sns.scatterplot(x=idf[\"Sepal Length\"], y=idf[\"Petal Length\"], hue=idf[\"name\"], style=km5.labels_, \n",
        "                s=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdjcoKgm0PsK"
      },
      "source": [
        "Versuch es nochmal mit der richtigen Anzahl der Cluster:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JHq7dlFF0PsK"
      },
      "source": [
        "km3 = KMeans(n_clusters=3)\n",
        "km3.fit(iris['data'])\n",
        "km3.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E0qLwyE0PsL"
      },
      "source": [
        "Das Ergebnis sieht jetzt auch optisch sehr viel besser aus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IC3tXnHO0PsL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(16,9))\n",
        "sns.scatterplot(x=idf[\"Sepal Length\"], y=idf[\"Petal Length\"], hue=idf[\"name\"], style=km3.labels_, \n",
        "                s=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpXdUbrH0PsL"
      },
      "source": [
        "Ein Clusterverfahren wie `MeanShift` ermittelt die Anzahl der Cluster selbstständig:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goL6CAIs0PsL"
      },
      "source": [
        "from sklearn.cluster import MeanShift\n",
        "ms = MeanShift()\n",
        "ms.fit(iris['data'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uEWCvxV0PsL"
      },
      "source": [
        "plt.figure(figsize=(16,9))\n",
        "sns.scatterplot(x=idf[\"Sepal Length\"], y=idf[\"Petal Length\"], hue=idf[\"name\"], style=ms.labels_, \n",
        "                s=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUcQJb020PsM"
      },
      "source": [
        "Mithilfe des Silhouette-Scores kannst du ermitteln, wie gut das Clustering (für *konvexe Cluster*) funktioniert hat:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhKRS3Do0PsM"
      },
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "silhouette_score(iris['data'], km3.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFxKg-n20PsM"
      },
      "source": [
        "silhouette_score(iris['data'], km5.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DKxmtlj0PsM"
      },
      "source": [
        "silhouette_score(iris['data'], ms.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiaOVCIvqvGI"
      },
      "source": [
        "km2 = KMeans(n_clusters=2)\r\n",
        "km2.fit(iris['data'])\r\n",
        "km2.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgAUPT_lqvbe"
      },
      "source": [
        "silhouette_score(iris['data'], km2.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEngxTdz0PsM"
      },
      "source": [
        "Innerhalb des gleichen Cluster-Verfahrens kannst du den Silhouette-Score gut vergleichen. `MeanShift` hat nach dem Bild oben natürlich auch gute Argument, warum es nur zwei Cluster gefunden hat. Vermutliche sind sich Spezies in dem \"oberen\" Cluster auch deutlich ähnlicher!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfzVtROf0PsN"
      },
      "source": [
        "### Reddit-Topic Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKiLnQMj0PsN"
      },
      "source": [
        "Topic-Modelle kann `scikit-learn` mit mehreren Methoden berechnen. `NMF` ist deutlich schneller als `LDA` und hat häufig ebenso gute Ergebnisse.\n",
        "\n",
        "Die Aufrufe sind sehr ähnlich zum Clustering und zur Klassifikation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-nT5nwP0PsN"
      },
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "num_topics = 10\n",
        "\n",
        "nmf = NMF(n_components = num_topics)\n",
        "nmf.fit(tfidf_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBQoVSGA0PsN"
      },
      "source": [
        "Um die Ergebnisse darzustellen, wollen wir Wordclouds benutzen. Dazu berechnest du die wichtigsten Wörter in jedem einzelnen Topic:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq5G4Hk90PsO"
      },
      "source": [
        "!pip install wordcloud\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def wordcloud_topic_model_summary(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        freq = { feature_names[i]: topic[i] for i in topic.argsort()[:-no_top_words - 1:-1] }\n",
        "        wc = WordCloud(background_color=\"white\", max_words=100, width=960, height=540)\n",
        "        wc.generate_from_frequencies(freq)\n",
        "        plt.figure(figsize=(12,12))\n",
        "        plt.imshow(wc, interpolation='bilinear')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "oRV8KB3d0PsO"
      },
      "source": [
        "wordcloud_topic_model_summary(nmf, tfidf_vectorizer.get_feature_names(), 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKGtsyaJ0PsO"
      },
      "source": [
        "Das Ergebnis ist gut interpretierbar und vor allem *unvoreingenommen*. Mit wenigen Zeilen Code hast du so die Themen in zwei Millionen Posts bestimmt!\n",
        "\n",
        "Die Anzahl der Topics kannst du variieren und sehen, ob du damit bessere Ergebnisse erzielen kannst. Es gibt auch Scores, mit denen du die Güte ermitteln kannst (Coherence Score oder Perplexity)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp-47Fm6EmQ9"
      },
      "source": [
        "## Zeitevolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpk36InZEIYz"
      },
      "source": [
        "docs[\"month\"] = pd.to_datetime(docs[\"created_utc\"], utc=True).dt.strftime(\"%Y-%m\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_iMrCRAErf0"
      },
      "source": [
        "import numpy as np\r\n",
        "month_data = []\r\n",
        "for month in np.unique(np.unique(docs[\"month\"])):\r\n",
        "    W_month = nmf.transform(tfidf_vectors[np.array(docs[\"month\"] == month)])\r\n",
        "    month_data.append([month] + list(W_month.sum(axis=0)/W_month.sum()*100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pki2mXcrE4mr"
      },
      "source": [
        "topic_names = []\r\n",
        "voc = tfidf_vectorizer.get_feature_names()\r\n",
        "for topic in nmf.components_:\r\n",
        "    important = topic.argsort()\r\n",
        "    top_word = voc[important[-1]] + \" \" + voc[important[-2]]\r\n",
        "    topic_names.append(\"Topic \" + top_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxFQi0OXFAvT"
      },
      "source": [
        "df_month = pd.DataFrame(month_data, columns=[\"month\"] + topic_names).set_index(\"month\")\r\n",
        "df_month.plot.area(figsize=(16,9))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}